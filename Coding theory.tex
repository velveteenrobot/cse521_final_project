\documentclass{article}
\author{Sarah Elliott,Mingrui Xu}
\title{Coding Theory and Application}
\usepackage{amsmath,amsthm ,cite , pdfpages, url}
\newtheorem{Definition}{Definition}
\newtheorem{Theorem}{Theorem}
\newtheorem{Lemma}{Lemma}
\newtheorem{Proposition}{Proposition}
\newtheorem{Proof}{Proof}

\begin{document}
\maketitle
\tableofcontents
\newpage

\section{Introduction}
The field of Coding Theory deals with problem of imperfect data transmission. When data is transmitted, part of the data might get corrupted, either erased or replaced by an incorrect (noise) value. The solution to such problems is to encode the data before transmission by adding some additional information, and when the receiver decodes the data, they should be able to detect the errors in the received code, and possibly have enough information to correct the data. Now it is clear that for some transmission methods, there does not exist any absolutely perfect code, as any data, including the ones added in encoding process, could be corrupted. But for well-characterized transmission methods, there might be a known distribution or upper bound on data corruption, hence there should be different coding strategies to deal with different transmission methods.

There are many objectives of a code, and the major ones include detecting and correcting a greater number of errors, using less additional space and having relatively small runtime for encoding and decoding. We can illustrate these objectives by using two typical toy example codes, triple code and parity. Assume the code we are sending is a sequence of bits 0 and 1, and random number of bits will be flipped during transmission due to noise. Triple code will triple every bit and send a coded message that is three times the original size. The receiver will decode the message by taking the majority bit of each triple. Such code can correct at most 1 error in one triple, and will have encode and decode time $O(n)$ where n is the number of bits in the original message. The other approach, parity code, will simply add one bit at the end of the sequence, which is the bit-wise XOR of all the previous bits. We can see that such code would only be able to detect one error from the original message and could not correct any errors. The encode and decode time will also be $O(n)$ since both the encoder and decoder will go through the whole sequence to check the parity. The advantage of the parity code will be it takes $O(1)$ space. The tradeoffs between these codes are that the first is able to detect and correct errors but takes up three times the original space. The second cannot correct any errors but takes only one additional bit of space.

We can see that in general we might want codes that correct as many errors as possible without adding too many addtional bits to the original message. We will expand further on desirable properties for a code. Unfortunately, it is not possible to just add one more bit and correct an arbitrary number of errors. There are well established bounds in Coding Theory that bound the relationship between the maximum number of errors that can be corrected and the minimum number of bits that must be added to the original data. Later, we will discuss the Hamming Bound, which is one such bound. We will also look at a couple of non-trivial examples of codes. We will focus on linear codes, in particular Hamming codes and Reed-Solomon codes. 

\section{Background}
\subsection{Definitions}

\noindent Here we provide some useful coding theory definitions, which are mostly summarized from \cite{book}.

\begin{Definition}
A code of block length n over an alphabet $\Sigma$ is a subset of $\Sigma ^n$. And $q$ denotes the size of alphabet $|\Sigma|$. \cite{book}
\end{Definition}
\noindent We often use the letter $C$ to represent a code as a map: $C: [M] \rightarrow \Sigma^n$ where $|C| = M$.

\begin{Definition}
Given a code $C \subseteq \Sigma^n$, we have that the dimension k is given by $k = log_q(|C|)$.
\end{Definition}
\noindent Here basically $k$ represents the size before encoding and $n$ denotes the total size of the code after encoding. 
\begin{Definition}
Rate of the code R is represented as $R=\frac{k}{n}$.
\end{Definition}
\noindent As stated above, we can analyze our two toy examples, the triple (repetition) and parity codes.  For the triple code, $R=\frac{1}{3}$ and for the parity code, $R = \frac{k}{k+1}$.
\begin{Definition}
An encoding function will be denoted as $E:|C| \rightarrow \Sigma^n$. Whereas the decoding function is written as $D:\Sigma^n \rightarrow |C|$.
\end{Definition}
\subsection{Distance of a Code}
Now as we have the basic definitions settled, we can start to explore how well a code can perform given certain constraints. We start from the definition of distance.
\begin{Definition}The Hamming distance for two different vectors u and v is given by $\bigtriangleup(u,v) = $ the number of bits in which u and v differ.
\end{Definition}
\noindent And with the definition of the Hamming distance we can have a definition of distance of a code.
\begin{Definition}
The distance of a code C is given by $d = min \bigtriangleup (u,v)$ where $u \neq v$.
\end{Definition}
\noindent The distance of a code is an important characteristic of a code, since it implies the number of errors and erasures the code could detect and correct. This relationship is given by the following proposition:
\begin{Proposition}The following statements are equivalent for a code C:\end{Proposition}
1. \quad C has minimum distance $d \ge 2$.\\
\indent 2. \quad If d is odd, C can correct $\frac{d-1}{2}$ errors(or $\frac{d}{2}-1$ errors if even).\\
\indent 3. \quad C can detect $d-1$ errors.\\
\indent 4. \quad C can correct $d-1$ erasures.\\
The proof for the above proposition is left in the book \cite{book}. And the result of this proposition is that we have an easier way to describe the number of errors that a code can detect or fix, just by using the simple parameter $d$. Now, as we know the definition of distance of a code, we can write a code as $(n,k,d)_\Sigma$ to represents this generalized property.
\subsection{Hamming Bound}
As we know the definition of distance, we can explore the upper bound of number of errors a code can fix. One of the most simple upper bound is given by
\begin{Theorem}(Hamming bound)\end{Theorem}
\noindent For every $(n,k,d)_{q}$ code, we have:
\begin{equation}
k \le n - log_q(\sum_{i=0}^{[\frac{d-1}{2}]}{n \choose i} (q-1)^i)
\end{equation}
To prove such a bound, we need to have a definition of a Hamming ball.
\begin{Definition}(Hamming ball)\end{Definition}
\noindent For any vector $x \in [q]^n$,
$B(x,e) = \{y \in [q]^n | \bigtriangleup (x,y) \le e\}$.\\
Basically this definition describes all the vectors(words) within Hamming distance $e$ of a certain vector $x$. You can picture a ``ball'' with radius $e$ and centered at $x$. All words within distance $e$ of $x$ would lie within the ball. With this definition we can prove the correctness of the Hamming bound.
\begin{Proof}Proof of Hamming Bound: \end{Proof} 
\noindent First set $e = \frac{d-1}{2}$. We choose this value for $e$ because that is the maximum number of errors for which a $minimum\, distance\, decoding$ will still correctly decode the transmission. More simply, if you decode a transmission by matching it to the code word that has the greatest number of matching bits, then the maximum number of errors in the transmission (for it to be decoded correctly) would have to be less than half the distance from the intended code word to the next closest code word. Otherwise, the transmission would be ``closest'' to the incorrect word. Since $d$ is the minimum distance between any two code words in the code,  $e = \frac{d-1}{2}$ gives the maximum number of errors for any transmission using that code. 

 Next, given any two codewords in C, $c_1 \neq c_2$, we have that $B(c_1,e) \cap B(c_2,e) = \emptyset$. This means that the two words have mutually exclusive sets of words that are within distance $e$ of each word. This makes sense since $c_1$ and $c_2$ are at least distance $d$ from each other, and the sum of radii of the Hamming Balls, $e$, is smaller than minimum distance $d$. 

We then claim that $|B(x,e)| = \sum_{i=0}^{e} {n \choose i}(q-i)^n$. Since for any word that has distance $i$ from $x$, we can choose ${n \choose i}$ different spots to differ, and on each different spot there are $q-i$ selections.

Now, as we have that $\bigcup B(c,e) \subset \Sigma^n$, we will have $|\bigcup B(c,e)| \le q^n$. On the other hand, we also have $|\bigcup B(c,e)| = \sum B(c,e) = q^k* \sum_{i=0}^{e} {n \choose i}(q-i)^n$. Combining the previous results we derive that $k \le n - log_q(\sum_{i=0}^{[\frac{d-1}{2}]}{n \choose i} (q-1)^i)$. We can see that the Hamming bound is a loose bound, and very few codes can be tight bounded by it. The only non-trivial ones are called Hamming Code and Golay Code.\\
\section{Linear Codes}
\subsection{Inroduction to Linear Codes}
Now we will discuss a class of nicely behaved codes called linear codes. Normally, if we have a entirely random coding strategy for a code $C:[q]^k \rightarrow [q]^n$, then for decoding purposes, we will need to store the whole code C in a dictionary, so every time the receiver receives a transmission he could look up the transmitted word in dictionary and choose the code word with the shortest distance from the word he received. The problem is that for $C:[q]^k \rightarrow [q]^n$, we have $[q]^k$ entries, and for each entry, the key occupies space $n$. Hence such a dictionary would have space performance of at least $nq^k$, which is exponential with respect to dimension $k$. We need a code with a structure such that the original word can be retrieved without storing a 1-to-1 mapping between coded and decoded words.
\begin{Definition}(Linear Codes)\end{Definition}
\noindent Let $q$ be a prime power, and $C \subseteq {0,1,2...q-1}^k$ is a linear code if it is a linear subspace of ${0,1,2...q-1}^k$. In this situation we denote the code $C$ with dimension $k$ and distance $d$ as $[n,d,k]_q$ code.

Here it is easy to see that in order to be a linear subspace, all linear combinations of code words in a code must also themselves be code words. As well, the zero code word must be in the code . For the binary field $\{0,1\}$, we realize that the addition of two numbers in the field is equivalent to the XOR operation, and that gives a hint that the parity code will be linear.

Now since the linear code is always a linear subspace, we can represent them in terms of matrices as we will show below.
\begin{Definition}(Generator Matrix)\end{Definition}
\noindent If $S \subseteq F_q^n$ is a linear subspace with dimension $k$, then there exists vectors $v_1, v_2...v_k$ such that any $x \in S$ would be represented by linear combination of $v_i$, namely $x = a_1v_1 + a_2v_2+ ... a_kv_k$ where $a_i \in F_q$. The generator matrix $G$ is a matrix such that for every $x \in S$, we have that $x = (a_1,a_2...a_n)*G$. Such matrix G will be the following: \\\\
G =
$\begin{bmatrix}
    \leftarrow       & v_1 & \rightarrow \\
    \leftarrow       & v_2 & \rightarrow \\
    \hdotsfor{3} \\
    \leftarrow       & v_k & \rightarrow
\end{bmatrix}$\\\\
Another way to interpret this is that the linear subspace of the code C could be represented by the "row space" of the Generator matrix G.
% TODO: Add some intuitive/comceptual explanation of the generator matrix?
\begin{Definition}(Parity Check Matrix)\end{Definition}
\noindent If $S \subseteq F_q^n$ is a linear subspace with dimension $k$, then there exist full rank (n-k)*n matrix $H$ such that for all $x \in S$, $Hx^T = 0$. And such matrix $H$ is called the parity check matrix for code $C$.

Similar to the idea of generator matrix, the parity check matrix also has its analogous idea in linear algebra: The linear subspace represented by code $C$ could be denoted as the ``null space'' of the parity check matrix $H$.

Now we can prove that the triple (repetition) code and the parity code are both linear codes, by writing down their Generator matrices.

Assuming we are applying the triple (repetition) code and parity codes to words from the binary field with dimension $k$. Then the generator matrix for triple code will have size $k \times 3k$ as the one below:\\\\
$G_{3,rep} = \begin{bmatrix}
    1 & 1 & 1 & 0 & 0 & 0 & 0 & \dots\\
    0 & 0 & 0 & 1 & 1 & 1 & 0 & \dots \\
    \hdotsfor{8} \\
    0 & 0 & \dots & 0 & 0 & 1 & 1 & 1 \\
\end{bmatrix}$\\\\ 
After the multiplication every bit will 'assign' 3 spots in the final encoded word. Similarly, based on the same assumption, we will have that the generator matrix for the parity code is the one below with size $k \times (k+1) $:\\\\
$G_\oplus = \begin{bmatrix}
    1 & 0& 0 & \dots & 0 & 1\\
    0 & 1& 0 & \dots & 0 & 1 \\
    \hdotsfor{6} \\
    0 & 0& 0 & \dots & 1 & 1 \\
\end{bmatrix}$\\\\
So both of our toy examples can be categorized as linear code. In fact, not just the toy examples, many useful non-trivial codes are linear, since linear codes behave nicely for decoding.
\subsection{Distance of Linear Codes}
To generalize the distance of linear code, we need to first define the weight of a codeword.
\begin{Definition}(Hamming weight)\end{Definition}
\noindent Let $q \ge 2$. For any vector $v \in {1,2...q-1}^n$, we have $wt(v)$ = the number of nonzero symbols in $v$.

Now with the definition of the Hamming weight, we can introduce the proposition of distance on linear codes.
\begin{Proposition} \end{Proposition}
\noindent For a $[n,k,d]_q$ linear code, $d =$ min $wt(c)$ for $c \neq 0$. The proof of this proposition is also in \cite{book}, and the basic idea is to prove that the minimum weight is both the upper bound and the lower bound of the distance.
\subsection{Families of Codes}
Since the dimension of the vector that needs to be encoded varies all the time, a code should be able to generalize to arbitrary dimensions. This is true of our toy examples, the parity and repetition codes.
\begin{Definition}(Family of codes)\end{Definition}
\noindent $C = C_i(i \ge 1)$ is a family of codes where $C_i$ is a $[n_i,k_i,d_i]_q$ code for each $i \ge 1$. And in this case, we define the rate R of the code to be $ R(C) = \lim_{i\to\infty} \frac{k_i}{n_i}$. And the relative distance of the code to be $\delta = \lim_{i\to\infty} \frac{d_i}{n_i}$.

Take our toy example, we will have that $R(C_{3,rep}) = \frac{1}{3}$ and $R(C_\oplus) = 1$. Both relative distances will be zero since $d$ is constant for either family of codes, and $n_i$ will go to $\infty$ as $i$ approaches $\infty$, hence $\lim_{i\to\infty} \frac{d_i}{n_i} = 0$ for both examples. The reason that $\delta = 0$ for $C_{3,rep}$ is that although it can correct more than one error under certain conditions, there is no guarantee that it can always correct more than one error in a transmission. As long as two errors occur in the same block, triple code would fail.

\section{Hamming Codes}
\subsection{Hamming (7-4) Code}
Now we will start with our first non-trivial code example, which is the family of Hamming codes based on the coding strategy proposed by Richard Hamming in 1950 \cite{hamming-code}. Hamming codes are a family of codes which can be generalized to higher dimensions, and we start from looking at one simple example from the family.

Hamming(7-4) code is a $(7,4,3)_2$ code, and it has the following Generator matrix: \\\\
$G = \begin{bmatrix}
	1 & 0 & 0 & 0 & 0 & 1 & 1 \\
	0 & 1 & 0 & 0 & 1 & 0 & 1 \\
	0 & 0 & 1 & 0 & 1 & 1 & 0  \\
	0 & 0 & 0 & 1 & 1 & 1 & 1 \\
    \end{bmatrix}$.\\\\
And from the generator matrix, we can find that the mapping of this code is simply 
$C: \{x_1, x_2, x_3,x_4\} \rightarrow \{x_1,x_2,x_3,x_4, x_2\oplus x_3\oplus x_4,x_2\oplus x_3\oplus x_4,x_1\oplus x_2\oplus x_4\}$.

Now we can do some simple analysis on this code. Using Proposition 2, we can get that $d = 3$. And we can check with Theorem 1 that for $d=3$, the Hamming(7-4) code is tight bounded by the Hamming bound. Consider Hamming code always has $d=3$ and $q=2$ since it's on the binary field, applying Theorem 1 we get the bound $k \le n-log_2({n\choose 0}*1^0 + {n\choose 1}*2^1) = n-log_2(n+1)$. Now plug in the value of $k=4$ and $n = 7$, we get that $k = n-log_2(n+1)$ for this code, and hence the Hamming code is tight bounded by the Hamming bound.

\subsection{Encoding Generalized Hamming Codes}
The generalized Hamming Code is a family of $[2^r-1, 2^r-r-1,3]_q$ code (here we only discuss the binary version where $q=2$), which are all tight bounded by Hamming bound. The general algorithm of Hamming code is the following:\\\\
\indent 1. \quad Among $2^r-1$ bits, choose all the bits at positions that are powers of 2 (namely 1,2,4...$2^{r-1}$) as parity bits. The other bits are message bits.\\
\indent 2. \quad Each of the parity bits calculate parity for a certain subset of the bits in the coded word. The parity bit with index 1 covers the bits 1,3,5,7. The parity bit with index 2 covers the bits 2-3, 6-7,10-11... The parity bits with index 4 covers the bits 4-7, 12-15,20,23... The basic pattern is that the parity bits $2^i$ covers all the bits in the interval $[2^i*(2k-1), 2^i*2k )$ where k is an integer.\\
\indent 3. \quad For all the message bits that a parity bit covers (this excludes the parity bit itself), the parity bit is simply the addition of all those message bits, which for a binary code is just the XOR of all the message bits it covers. 

For the actual implementation, we can first map $2^r-r-1$ message bits to the correct positions, and then for each of the parity bit, compute the XORs. For the runtime analysis, we find that mapping the message bits cost $O(2^r)$ time, and since each parity bit covers roughly half of the entire vector, computing parity cost $O(r*2^{r-1})$ time. Considering the actually dimension $k$ is in $O(2^r)$, we have that the actual encoding time is $O(k \;logk)$ with respect to dimension $k$.
\subsection{Decoding Generalized Hamming Codes}
For the general decoding of linear codes, we can consider two basic approaches, the $maximum\, likelihood\, decoder$ and the $minimum\, distance\, decoder$. The maximum likelihood decoder, as defined by \cite{decoder_citation}, returns a code word $c_1$ that maximizes $P( c_2\, received | c_1\, sent)$ for a received word $c_2$, $c_1, c_2 \in C$. This can be modeled as an integer programming problem, but due to complexity we will not consider it further. 

For our second method, w henever a transmitted word is received, the minimum distance decoder returns the code word that is closest to the received word in terms of Hamming distance. Now the generalized minimum distance decoder strategy, as stated in Section 2, will need exponential space to store all the possible codewords, and hence in general this method is prohibitive and could not be used in practice.

Now, linear codes all have nice properties to facilitate fast decoding. An important property is that for the parity check matrix,  $vH^T = 0$. We can use this fact to develop a decoding method called $syndrome\, decoding$.\\
\begin{Definition}(Syndrome)\end{Definition}
\noindent For any given vector x, the syndrome of the vector is defined as $s(x) = xH^T$.

Suppose the original encoded message was $v$, and after sending through the channel it gets added to an error message $e$ and the receiver gets the message $x = v+e$. Then we have that $xH^T = (v+e)H^T = vH^T+eH^T = eH^T$. 

We will assume that $e \leq \lfloor \frac{d - 1}{2} \rfloor$, otherwise the Hamming Code cannot detect the errors. Next we find all the possible error patterns with $e \leq \lfloor \frac{d - 1}{2} \rfloor$, compute all their syndromes, $ \sum_{i=0}^{t}{n \choose i} $ of them, and store them in a table to look up. Now when we have the table, for each syndrome we can look up the error $e_i$ and subtract it from the originally received message $x$ to get the code word. And once we get the correct code word, we can scan the correct message by taking all the data bits. For the runtime analysis, if we have all the syndrome stored in a table, we can easily look up the correct code word in the table in $O(1)$ time, and then use $O(k)$ time to scan all the data bits to get the message. So the expect runtime in this situation is simply $O(k)$.\\
\subsection{Block Code}
From the syndrome method above we find that when $r$ gets bigger and bigger, although we have linear expected runtime, we still need to store a large syndrome table. Although it is not exponentially increasing, the space still gets large when $k$ increases. One way to deal with this situation is to cut the original message into small blocks, and apply the same coding strategy onto each block. That way we can reuse our small syndrome table and avoid taking up a large amount of space. For example, to send a 16-bit data message, we could cut into 4 blocks of 4-bit data messages, and apply  Hamming(7-4) on each block. Such a strategy is called block code and is used a lot in practice.
\section{Reed-Solomon Codes}
\subsection{Introduction to Reed-Solomon Codes} 
Reed-Solomon (RS) codes are a set of codes that have been widely used in different applications. They were introduced by Irving S. Reed and Gustave Solomon in 1960 \cite{reed-solomon}. RS codes satisfy what is known as the Singleton Bound. We will not discuss the Singleton bound in depth here but, it basically states that the sum of the rate and the relative distance are not much bigger than one \cite{singleton}:

\begin{equation}
R + \delta \leq 1 + \frac{1}{n}
\end{equation}

\noindent This can also be stated as:

\begin{equation}
d \le n-k+1
\end{equation}

\noindent RS codes have the property that $d = n-k+1$.  RS codes have broad applications in data transmission, bar code technology and data storage technologies. The general idea of an RS code is the following:\\

Consider the data message $m$ with dimension $k$, $\{m_0, m_1,m_2...m_{k-1}\}_q$. We can map this to a polynomial of degree $k-1$ on the field $q$. The data characters $m_0, m_1 ...m_{k-1}$ are coefficients of $X^0, X^1...X^{k-1}$. Now, with the above transformation, we can write the data vector as a polynomial:

\begin{equation}
 P = \sum_{i=0}^{k-1}m_iX^i
\end{equation}

Now, for encoding, we choose $\alpha_1, \alpha_2...\alpha_n$ as $n$ different evaluating points, with $k \le n \le q$, and then evaluate the polynomial at those $\alpha_i$'s, and use those values as our codewords. Our codeword will be $\{P(\alpha_1), P(\alpha_2) , ... ,P(\alpha_n)\}$. The evaluating points $\alpha_i$ are known to both encoder and decoder in advance, and a common choice for them is to choose $F_q - \{0\}$ which would maximize the distance.

Now as stated above $k \le n \le q$, since if $k >n$ then we lose data after encoding, and if $n > q$, then we have repetitions in the evaluating points which waste space. If we use the common choice for evaluating points, $F_q - \{0\}$, RS code over a binary field would be useless since both $k$ and $n$ will be one and the  mapping would clearly be identity mapping. Also it is easy to see that RS code works better for larger fields than for extremely small ones.

We can look at one example of an RS code. Suppose we are dealing with a $[4,3,2]_5$ RS code. If the message vector we are sending is ${4, 2, 1}$, then we will have our polynomial P to be $P = X^2 + 2X + 4$, and using the common choice of evaluating points, which is ${1,2,3,4}$ in this case , we will get our encoded message as ${2, 2, 4,4}$.

One huge advantage that RS code has over the other codes is that it's easy for people to control the rate $R$ by adjusting the distance. By adjusting the encoded block length $k$ and the distance $d$ we can design suitable RS codes for different transmission environments. Compared with Hamming code, the distance could vary on the choice of $k$ instead of being fixed at a constant.
\subsection{Simple Encoding}
The simplest encoding strategy, as presented in Reed and Solomon's original paper, is stated as above. Furthermore, if we are using the common choice of evaluating points, we will have the following generator matrix:\\

$G = \begin{bmatrix}
	1 & 1 & 1 &\dots & 1 \\
	\alpha_1 & \alpha_2 & \alpha_3 & \dots & \alpha_n \\
	\alpha_1^2 & \alpha_2^2 & \alpha_3^2 & \dots & \alpha_n^2  \\
	\hdotsfor{5} \\
	\alpha_1^k & \alpha_2^k & \alpha_3^k & \dots & \alpha_n^k \\
    \end{bmatrix}$.\\

\noindent Such a matrix is also called the $Vandermonde$ matrix and it shows that RS code is also categorized as a linear code. For the runtime analysis, we notice that all operations are just evaluating polynomials, for each of the $n$ polynomials we have $k$ terms to evaluate. Therefore the runtime would be $O(nk)$ for this encoding method.
\subsection{A Different Encoder}
Now besides the above encoding method, there's another encoder, which sends entirely different messages to the receiver. In this new encoder, after computing the polynomial $P$, instead of evaluating on different points, we multiply the polynomial by another polynomial $g(X)$ to get to a new polynomial of degree $n-1$. We then send the $n$ coefficients of the new polynomial, we will call this the message polynomial, as the encoded message. The polynomial $g(X)$ is defined as follows:
\begin{Definition}(Generator Polynomial)\end{Definition}
\noindent The generator polynomial is defined as $g(X) = (x-\alpha)(x-\alpha^2)...(x-\alpha^{n-k})$.\\\\
Now this $g(X)$ is known in advance to both the sender and the receiver, and using this encoding method will turn the RS code to so called BCH code, which has its own generalized decoder.

\subsection{Decoding Reed-Solomon}
\noindent In their original paper, Reed and Solomon propose a decoding strategy. Their proposed decoder would consider subsets of $k$ symbols from the $n$ received symbols and  interpolate a possible message polynomial for each subset \cite{reed-solomon}. Whichever is the most popular message produced by the candidate polynomials would be considered to be the corrected message. Unfortunately this solution is not efficient as there are many subsets, ${n \choose k}$.

For our implementation of RS, we chose to use Syndrome decoding. In the case of RS it is slightly different than for the Hamming codes. In this case, you evaluate the message polynomial at the zeroes of the generator polynomial. If the message does not contain any errors then the result should be zero because the message polynomial is a multiple of the generator polynomial. 

There are techniques for handling both erasures and errors in the transmitted data. A description for how to do both is described in \cite{wikiversity}. We will focus on just correcting errors as that is the more common use case. In the real world, RS is used a lot in QR codes and QR code scanners are not always able to detect missing data anyway. An example of this would be if part of the QR code were covered. In this case, technically that data is missing, but many scanners would just interpret the covering material as being part of the code and use error correction to get the actual message.

To correct errors, we first use the Berlekamp-Massey algorithm to construct the aptly-named ``error locator polynomial''. The details of the process are beyond the scope of this paper, but the important intuition is that like with the Hamming codes, the error location information can be derived from the syndrome. Full details and implementation can be found here \cite{wikiversity}. Once we have the error locator polynomial, we find the zeroes of this polynomial, which give the error locations. If the polynomial is linear or quadratic there are efficient ways to solve it. Otherwise, trial and error is generally used. Once you know the locations of the errors, the Forney algorithm can be used to find the correct values for those locations. This algorithm also uses information from the syndrome but is beyond the scope of this paper. Full details and implementation can be found here \cite{wikiversity}.

%\subsection{Practical applications of Reed-Solomon Codes}
%\noindent 

\bibliographystyle{plain}
\bibliography{coding}

\end{document}
